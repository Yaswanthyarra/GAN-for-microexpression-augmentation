{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv7nj0bEmn3GtGzkVjLfEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yaswanthyarra/GAN-for-microexpression-augmentation/blob/main/GoogleNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8vEiCvDMRqa",
        "outputId": "510c02ad-fe57-4cf3-cd7c-ec0dc9208f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_dir = '/content/drive/My Drive/CASMEII/Final_Apex_Frames_Segregated'\n",
        "augmented_dir='/content/drive/My Drive/CASMEII/Augmented_Apex_Frames'\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset_dir = '/content/drive/My Drive/CASMEII/Final_Apex_Frames_Segregated'\n",
        "\n",
        "label_dict = {'happiness': 0, 'disgust': 1, 'repression': 2, 'surprise': 3, 'fear': 4, 'others': 6, 'sadness': 5}\n",
        "\n",
        "expression_images = []\n",
        "expression_labels = []\n",
        "\n",
        "for class_name, label in label_dict.items():\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    for image_file in images:\n",
        "        image_path = os.path.join(class_dir, image_file)\n",
        "        image = cv2.imread(image_path)  # Load the image using OpenCV\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image = cv2.resize(image, (224,224))\n",
        "        if label==1:\n",
        "          expression_images.append(image)\n",
        "          expression_labels.append(label)\n",
        "        if label==6:\n",
        "          expression_images.append(image)\n",
        "          expression_labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "        expression_images.append(image)\n",
        "        expression_labels.append(label)\n",
        "\n",
        "augmented_dir='/content/drive/My Drive/CASMEII/Augmented_Apex_Frames'\n",
        "label_dict = {'happiness': 0, 'repression': 2, 'surprise': 3, 'fear': 4, 'sadness': 5}\n",
        "\n",
        "for class_name, label in label_dict.items():\n",
        "    class_dir = os.path.join(augmented_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    for image_file in images:\n",
        "        image_path = os.path.join(class_dir, image_file)\n",
        "        image = cv2.imread(image_path)  # Load the image using OpenCV\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        if label==0:\n",
        "          expression_images.append(image)\n",
        "          expression_labels.append(label)\n",
        "\n",
        "        expression_images.append(image)\n",
        "        expression_labels.append(label)\n",
        "\n",
        "\n",
        "        expression_images.append(image)\n",
        "        expression_labels.append(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Assuming your expression_images array has shape (num_images, height, width, channels)\n",
        "\n",
        "# Resize images to a consistent shape\n",
        "target_shape = (224, 224)\n",
        "\n",
        "resized_images = []\n",
        "for img in expression_images:\n",
        "    # Ensure the image has the correct shape and data type\n",
        "    img = np.squeeze(img)  # Remove singleton dimensions\n",
        "    img = img.astype('uint8')  # Convert to unsigned integer type\n",
        "    img = Image.fromarray(img)\n",
        "\n",
        "    # Resize the image\n",
        "    img = img.resize(target_shape, Image.ANTIALIAS)\n",
        "    resized_images.append(np.array(img))\n",
        "\n",
        "expression_images = np.stack(resized_images)\n",
        "\n",
        "# Ensure consistent data type\n",
        "expression_images = expression_images.astype('float32')\n",
        "\n",
        "# Rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXA-XhH3Qpxl",
        "outputId": "1e89fc5f-fe07-4159-b408-09c257e80ad1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-aaf2c4bf8ca3>:17: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  img = img.resize(target_shape, Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "expression_images = np.array(expression_images)\n",
        "expression_labels = np.array(expression_labels)\n",
        "\n",
        "# Check the shape of the image data and labels\n",
        "print(\"Expression Images Shape:\", expression_images.shape)\n",
        "print(\"Expression Labels Shape:\", expression_labels.shape)\n",
        "\n",
        "# Obtain the number of class labels\n",
        "num_classes = len(np.unique(expression_labels))\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "expression_labels_one_hot = to_categorical(expression_labels, num_classes=num_classes)\n",
        "\n",
        "# Stratified Splitting of data into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    expression_images, expression_labels_one_hot, test_size=0.2, stratify=expression_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Load the InceptionV3 model pre-trained on ImageNet data\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the InceptionV3 base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add a global average pooling layer to reduce spatial dimensions\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Add a dense layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add the output layer with softmax activation for classification\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=52, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu6pOi3-QsD9",
        "outputId": "46f6f728-8351-4aee-b4d6-f213387c64be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression Images Shape: (1083, 224, 224, 3)\n",
            "Expression Labels Shape: (1083,)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22329127 (85.18 MB)\n",
            "Trainable params: 526343 (2.01 MB)\n",
            "Non-trainable params: 21802784 (83.17 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/52\n",
            "22/22 [==============================] - 137s 6s/step - loss: 23.1894 - accuracy: 0.2659 - val_loss: 4.4108 - val_accuracy: 0.4483\n",
            "Epoch 2/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 2.9582 - accuracy: 0.5231 - val_loss: 2.0656 - val_accuracy: 0.5287\n",
            "Epoch 3/52\n",
            "22/22 [==============================] - 128s 6s/step - loss: 1.2536 - accuracy: 0.6850 - val_loss: 1.3308 - val_accuracy: 0.6609\n",
            "Epoch 4/52\n",
            "22/22 [==============================] - 128s 6s/step - loss: 1.2104 - accuracy: 0.6879 - val_loss: 1.7519 - val_accuracy: 0.6264\n",
            "Epoch 5/52\n",
            "22/22 [==============================] - 128s 6s/step - loss: 1.0690 - accuracy: 0.7225 - val_loss: 0.6872 - val_accuracy: 0.7816\n",
            "Epoch 6/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 0.8013 - accuracy: 0.7876 - val_loss: 0.9845 - val_accuracy: 0.7184\n",
            "Epoch 7/52\n",
            "22/22 [==============================] - 110s 5s/step - loss: 0.6436 - accuracy: 0.8078 - val_loss: 0.6973 - val_accuracy: 0.7586\n",
            "Epoch 8/52\n",
            "22/22 [==============================] - 110s 5s/step - loss: 0.5279 - accuracy: 0.8382 - val_loss: 0.5798 - val_accuracy: 0.8046\n",
            "Epoch 9/52\n",
            "22/22 [==============================] - 110s 5s/step - loss: 0.6327 - accuracy: 0.8049 - val_loss: 0.7254 - val_accuracy: 0.7644\n",
            "Epoch 10/52\n",
            "22/22 [==============================] - 127s 6s/step - loss: 0.7506 - accuracy: 0.7948 - val_loss: 0.7369 - val_accuracy: 0.7586\n",
            "Epoch 11/52\n",
            "22/22 [==============================] - 127s 6s/step - loss: 0.6879 - accuracy: 0.7818 - val_loss: 0.7354 - val_accuracy: 0.7414\n",
            "Epoch 12/52\n",
            "22/22 [==============================] - 128s 6s/step - loss: 0.6498 - accuracy: 0.8295 - val_loss: 0.5732 - val_accuracy: 0.8391\n",
            "Epoch 13/52\n",
            "22/22 [==============================] - 109s 5s/step - loss: 0.9430 - accuracy: 0.7572 - val_loss: 0.7080 - val_accuracy: 0.8218\n",
            "Epoch 14/52\n",
            "22/22 [==============================] - 126s 6s/step - loss: 0.6458 - accuracy: 0.8266 - val_loss: 0.7526 - val_accuracy: 0.8218\n",
            "Epoch 15/52\n",
            "22/22 [==============================] - 111s 5s/step - loss: 0.6523 - accuracy: 0.8251 - val_loss: 0.6042 - val_accuracy: 0.7931\n",
            "Epoch 16/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 0.5778 - accuracy: 0.8165 - val_loss: 0.5149 - val_accuracy: 0.8218\n",
            "Epoch 17/52\n",
            "22/22 [==============================] - 110s 5s/step - loss: 0.4912 - accuracy: 0.8468 - val_loss: 0.8031 - val_accuracy: 0.8103\n",
            "Epoch 18/52\n",
            "22/22 [==============================] - 126s 6s/step - loss: 0.5696 - accuracy: 0.8454 - val_loss: 0.6332 - val_accuracy: 0.8391\n",
            "Epoch 19/52\n",
            "22/22 [==============================] - 128s 6s/step - loss: 0.5925 - accuracy: 0.8165 - val_loss: 0.5503 - val_accuracy: 0.8678\n",
            "Epoch 20/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.6432 - accuracy: 0.8092 - val_loss: 0.9751 - val_accuracy: 0.7356\n",
            "Epoch 21/52\n",
            "22/22 [==============================] - 113s 5s/step - loss: 0.5140 - accuracy: 0.8454 - val_loss: 1.1261 - val_accuracy: 0.6954\n",
            "Epoch 22/52\n",
            "22/22 [==============================] - 129s 6s/step - loss: 1.1316 - accuracy: 0.7327 - val_loss: 1.4927 - val_accuracy: 0.6667\n",
            "Epoch 23/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.9089 - accuracy: 0.7775 - val_loss: 1.1130 - val_accuracy: 0.8046\n",
            "Epoch 24/52\n",
            "22/22 [==============================] - 129s 6s/step - loss: 0.8881 - accuracy: 0.7934 - val_loss: 1.1389 - val_accuracy: 0.7069\n",
            "Epoch 25/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.4867 - accuracy: 0.8526 - val_loss: 0.6015 - val_accuracy: 0.8103\n",
            "Epoch 26/52\n",
            "22/22 [==============================] - 133s 6s/step - loss: 0.4682 - accuracy: 0.8598 - val_loss: 0.7108 - val_accuracy: 0.8103\n",
            "Epoch 27/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.4343 - accuracy: 0.8699 - val_loss: 1.0953 - val_accuracy: 0.7701\n",
            "Epoch 28/52\n",
            "22/22 [==============================] - 132s 6s/step - loss: 0.4741 - accuracy: 0.8569 - val_loss: 0.5727 - val_accuracy: 0.8103\n",
            "Epoch 29/52\n",
            "22/22 [==============================] - 132s 6s/step - loss: 0.3168 - accuracy: 0.8974 - val_loss: 0.4567 - val_accuracy: 0.8563\n",
            "Epoch 30/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 0.3583 - accuracy: 0.8772 - val_loss: 0.5186 - val_accuracy: 0.8621\n",
            "Epoch 31/52\n",
            "22/22 [==============================] - 132s 6s/step - loss: 0.3888 - accuracy: 0.8685 - val_loss: 0.8025 - val_accuracy: 0.7356\n",
            "Epoch 32/52\n",
            "22/22 [==============================] - 113s 5s/step - loss: 0.4326 - accuracy: 0.8743 - val_loss: 0.6259 - val_accuracy: 0.7931\n",
            "Epoch 33/52\n",
            "22/22 [==============================] - 131s 6s/step - loss: 0.3312 - accuracy: 0.8902 - val_loss: 0.6042 - val_accuracy: 0.8621\n",
            "Epoch 34/52\n",
            "22/22 [==============================] - 131s 6s/step - loss: 0.3796 - accuracy: 0.8858 - val_loss: 0.8636 - val_accuracy: 0.7701\n",
            "Epoch 35/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 0.5548 - accuracy: 0.8439 - val_loss: 0.9504 - val_accuracy: 0.8218\n",
            "Epoch 36/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 0.7753 - accuracy: 0.7934 - val_loss: 1.3165 - val_accuracy: 0.7759\n",
            "Epoch 37/52\n",
            "22/22 [==============================] - 132s 6s/step - loss: 0.5404 - accuracy: 0.8425 - val_loss: 0.6046 - val_accuracy: 0.8506\n",
            "Epoch 38/52\n",
            "22/22 [==============================] - 132s 6s/step - loss: 0.5499 - accuracy: 0.8309 - val_loss: 1.3409 - val_accuracy: 0.7701\n",
            "Epoch 39/52\n",
            "22/22 [==============================] - 131s 6s/step - loss: 0.7174 - accuracy: 0.8382 - val_loss: 0.9883 - val_accuracy: 0.7471\n",
            "Epoch 40/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.6125 - accuracy: 0.8237 - val_loss: 1.9884 - val_accuracy: 0.5862\n",
            "Epoch 41/52\n",
            "22/22 [==============================] - 129s 6s/step - loss: 0.9695 - accuracy: 0.7861 - val_loss: 0.5866 - val_accuracy: 0.8448\n",
            "Epoch 42/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.6429 - accuracy: 0.8410 - val_loss: 0.5602 - val_accuracy: 0.8448\n",
            "Epoch 43/52\n",
            "22/22 [==============================] - 113s 5s/step - loss: 0.4590 - accuracy: 0.8497 - val_loss: 0.4726 - val_accuracy: 0.8563\n",
            "Epoch 44/52\n",
            "22/22 [==============================] - 130s 6s/step - loss: 0.5852 - accuracy: 0.8396 - val_loss: 0.8890 - val_accuracy: 0.8218\n",
            "Epoch 45/52\n",
            "22/22 [==============================] - 131s 6s/step - loss: 0.7990 - accuracy: 0.8136 - val_loss: 0.4630 - val_accuracy: 0.8448\n",
            "Epoch 46/52\n",
            "22/22 [==============================] - 113s 5s/step - loss: 0.3187 - accuracy: 0.8902 - val_loss: 0.8446 - val_accuracy: 0.7816\n",
            "Epoch 47/52\n",
            "22/22 [==============================] - 132s 6s/step - loss: 0.3599 - accuracy: 0.8931 - val_loss: 0.4200 - val_accuracy: 0.8678\n",
            "Epoch 48/52\n",
            "22/22 [==============================] - 131s 6s/step - loss: 0.5432 - accuracy: 0.8353 - val_loss: 0.9862 - val_accuracy: 0.8218\n",
            "Epoch 49/52\n",
            "22/22 [==============================] - 113s 5s/step - loss: 0.4903 - accuracy: 0.8512 - val_loss: 0.8275 - val_accuracy: 0.8391\n",
            "Epoch 50/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.4215 - accuracy: 0.8613 - val_loss: 0.6208 - val_accuracy: 0.8276\n",
            "Epoch 51/52\n",
            "22/22 [==============================] - 129s 6s/step - loss: 0.2806 - accuracy: 0.8988 - val_loss: 0.4841 - val_accuracy: 0.8506\n",
            "Epoch 52/52\n",
            "22/22 [==============================] - 112s 5s/step - loss: 0.2486 - accuracy: 0.9191 - val_loss: 0.7669 - val_accuracy: 0.8506\n",
            "7/7 [==============================] - 30s 4s/step - loss: 0.6972 - accuracy: 0.8295\n",
            "Test accuracy: 0.8294931054115295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "expression_images = np.array(expression_images)\n",
        "expression_labels = np.array(expression_labels)\n",
        "\n",
        "# Check the shape of the image data and labels\n",
        "print(\"Expression Images Shape:\", expression_images.shape)\n",
        "print(\"Expression Labels Shape:\", expression_labels.shape)\n",
        "\n",
        "# Obtain the number of class labels\n",
        "num_classes = len(np.unique(expression_labels))\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "expression_labels_one_hot = to_categorical(expression_labels, num_classes=num_classes)\n",
        "\n",
        "# Stratified Splitting of data into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    expression_images, expression_labels_one_hot, test_size=0.2, stratify=expression_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Load the InceptionV3 model pre-trained on ImageNet data\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the InceptionV3 base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add a global average pooling layer to reduce spatial dimensions\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Add a dense layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add the output layer with softmax activation for classification\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wXEFzKEPlYu",
        "outputId": "34edc606-cfbc-4a4b-9e17-65c17c7383be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression Images Shape: (1083, 224, 224, 3)\n",
            "Expression Labels Shape: (1083,)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22329127 (85.18 MB)\n",
            "Trainable params: 526343 (2.01 MB)\n",
            "Non-trainable params: 21802784 (83.17 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.save('/content/drive/My Drive/GAN_Models/GoogleNet_Microexp.h5')\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/My Drive/GAN_Models/GoogleNet_Microexp.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVPTxACnU-sr",
        "outputId": "e0c9b8d9-a24d-4910-cc9c-d59e5817c8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/GAN_Models/GoogleNet_Microexp.h5')\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
        "\n",
        "# ... (previous code for model training and evaluation)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_probabilities = model.predict(test_images)\n",
        "predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "# Convert one-hot encoded test labels back to categorical labels\n",
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "# Print accuracy, precision, recall, F1-score, and UAR\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "print(f'Precision: {precision_score(true_labels, predicted_labels, average=\"weighted\"):.4f}')\n",
        "print(f'Recall: {recall_score(true_labels, predicted_labels, average=\"weighted\"):.4f}')\n",
        "print(f'F1-score: {f1_score(true_labels, predicted_labels, average=\"weighted\"):.4f}')\n",
        "print(f'Unweighted Average Recall (UAR): {balanced_accuracy_score(true_labels, predicted_labels):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRRaqDfsPNhQ",
        "outputId": "f51cfbd9-96e0-4477-9f00-727bd1c0bc63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 28s 4s/step\n",
            "7/7 [==============================] - 32s 4s/step - loss: 0.6972 - accuracy: 0.8295\n",
            "Test Accuracy: 0.8295\n",
            "Precision: 0.8464\n",
            "Recall: 0.8295\n",
            "F1-score: 0.8314\n",
            "Unweighted Average Recall (UAR): 0.8277\n"
          ]
        }
      ]
    }
  ]
}