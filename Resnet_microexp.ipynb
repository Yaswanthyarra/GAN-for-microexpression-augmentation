{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrxlGbsW1lREo+1CVtKYvU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yaswanthyarra/GAN-for-microexpression-augmentation/blob/main/Resnet_microexp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n-agMXULiXT",
        "outputId": "4eb21b49-70fa-43ce-bda8-de8e32007119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_dir = '/content/drive/My Drive/CASMEII/Final_Apex_Frames_Segregated'\n",
        "augmented_dir='/content/drive/My Drive/CASMEII/Augmented_Apex_Frames'\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset_dir = '/content/drive/My Drive/CASMEII/Final_Apex_Frames_Segregated'\n",
        "\n",
        "label_dict = {'happiness': 0, 'disgust': 1, 'repression': 2, 'surprise': 3, 'fear': 4, 'others': 6, 'sadness': 5}\n",
        "\n",
        "expression_images = []\n",
        "expression_labels = []\n",
        "\n",
        "for class_name, label in label_dict.items():\n",
        "    class_dir = os.path.join(dataset_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    for image_file in images:\n",
        "        image_path = os.path.join(class_dir, image_file)\n",
        "        image = cv2.imread(image_path)  # Load the image using OpenCV\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image = cv2.resize(image, (224,224))\n",
        "        if label==1:\n",
        "          expression_images.append(image)\n",
        "          expression_labels.append(label)\n",
        "        if label==6:\n",
        "          expression_images.append(image)\n",
        "          expression_labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "        expression_images.append(image)\n",
        "        expression_labels.append(label)\n",
        "\n",
        "augmented_dir='/content/drive/My Drive/CASMEII/Augmented_Apex_Frames'\n",
        "label_dict = {'happiness': 0, 'repression': 2, 'surprise': 3, 'fear': 4, 'sadness': 5}\n",
        "\n",
        "for class_name, label in label_dict.items():\n",
        "    class_dir = os.path.join(augmented_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    for image_file in images:\n",
        "        image_path = os.path.join(class_dir, image_file)\n",
        "        image = cv2.imread(image_path)  # Load the image using OpenCV\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        if label==0:\n",
        "          expression_images.append(image)\n",
        "          expression_labels.append(label)\n",
        "\n",
        "        expression_images.append(image)\n",
        "        expression_labels.append(label)\n",
        "\n",
        "\n",
        "        expression_images.append(image)\n",
        "        expression_labels.append(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Assuming your expression_images array has shape (num_images, height, width, channels)\n",
        "\n",
        "# Resize images to a consistent shape\n",
        "target_shape = (224, 224)\n",
        "\n",
        "resized_images = []\n",
        "for img in expression_images:\n",
        "    # Ensure the image has the correct shape and data type\n",
        "    img = np.squeeze(img)  # Remove singleton dimensions\n",
        "    img = img.astype('uint8')  # Convert to unsigned integer type\n",
        "    img = Image.fromarray(img)\n",
        "\n",
        "    # Resize the image\n",
        "    img = img.resize(target_shape, Image.ANTIALIAS)\n",
        "    resized_images.append(np.array(img))\n",
        "\n",
        "expression_images = np.stack(resized_images)\n",
        "\n",
        "# Ensure consistent data type\n",
        "expression_images = expression_images.astype('float32')\n",
        "\n",
        "# Rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIs_f9zhOMIB",
        "outputId": "4ce118eb-1fea-4a65-b4d6-87731ab983b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-aaf2c4bf8ca3>:17: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  img = img.resize(target_shape, Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming expression_images has shape (num_samples, height, width, channels)\n",
        "# and expression_labels are the corresponding labels\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "expression_images = np.array(expression_images)\n",
        "expression_labels = np.array(expression_labels)\n",
        "\n",
        "# Check the shape of the image data and labels\n",
        "print(\"Expression Images Shape:\", expression_images.shape)\n",
        "print(\"Expression Labels Shape:\", expression_labels.shape)\n",
        "\n",
        "# Obtain the number of class labels\n",
        "num_classes = len(np.unique(expression_labels))\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "expression_labels_one_hot = to_categorical(expression_labels, num_classes=num_classes)\n",
        "\n",
        "# Stratified Splitting of data into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    expression_images, expression_labels_one_hot, test_size=0.2, stratify=expression_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Load the ResNet50 model pre-trained on ImageNet data (excluding the top layers)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the ResNet base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add a global average pooling layer to reduce spatial dimensions\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Add a dense layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add the output layer with softmax activation for classification\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=52, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6NHLUYhONwi",
        "outputId": "0d37e18f-be8a-4d0c-f560-d5558d4cd4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expression Images Shape: (1083, 224, 224, 3)\n",
            "Expression Labels Shape: (1083,)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24114055 (91.99 MB)\n",
            "Trainable params: 526343 (2.01 MB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/52\n",
            "22/22 [==============================] - 185s 8s/step - loss: 0.8554 - accuracy: 0.7197 - val_loss: 0.5008 - val_accuracy: 0.8391\n",
            "Epoch 2/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.4834 - accuracy: 0.8425 - val_loss: 0.5276 - val_accuracy: 0.8563\n",
            "Epoch 3/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.4343 - accuracy: 0.8439 - val_loss: 0.6395 - val_accuracy: 0.8333\n",
            "Epoch 4/52\n",
            "22/22 [==============================] - 181s 8s/step - loss: 0.3979 - accuracy: 0.8439 - val_loss: 0.4374 - val_accuracy: 0.8563\n",
            "Epoch 5/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.3130 - accuracy: 0.8916 - val_loss: 0.4179 - val_accuracy: 0.8851\n",
            "Epoch 6/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.2844 - accuracy: 0.9032 - val_loss: 0.3658 - val_accuracy: 0.8851\n",
            "Epoch 7/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.2558 - accuracy: 0.9191 - val_loss: 0.3909 - val_accuracy: 0.8506\n",
            "Epoch 8/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.2405 - accuracy: 0.9176 - val_loss: 0.3842 - val_accuracy: 0.8678\n",
            "Epoch 9/52\n",
            "22/22 [==============================] - 178s 8s/step - loss: 0.2178 - accuracy: 0.9306 - val_loss: 0.3474 - val_accuracy: 0.8908\n",
            "Epoch 10/52\n",
            "22/22 [==============================] - 178s 8s/step - loss: 0.2223 - accuracy: 0.9220 - val_loss: 0.5565 - val_accuracy: 0.7816\n",
            "Epoch 11/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.2199 - accuracy: 0.9176 - val_loss: 0.4440 - val_accuracy: 0.8736\n",
            "Epoch 12/52\n",
            "22/22 [==============================] - 181s 8s/step - loss: 0.1692 - accuracy: 0.9465 - val_loss: 0.3567 - val_accuracy: 0.8793\n",
            "Epoch 13/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.1674 - accuracy: 0.9364 - val_loss: 0.3849 - val_accuracy: 0.8448\n",
            "Epoch 14/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.1898 - accuracy: 0.9277 - val_loss: 0.3143 - val_accuracy: 0.8736\n",
            "Epoch 15/52\n",
            "22/22 [==============================] - 177s 8s/step - loss: 0.2414 - accuracy: 0.9032 - val_loss: 0.3945 - val_accuracy: 0.8621\n",
            "Epoch 16/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.2025 - accuracy: 0.9234 - val_loss: 0.4227 - val_accuracy: 0.8448\n",
            "Epoch 17/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.1222 - accuracy: 0.9552 - val_loss: 0.3393 - val_accuracy: 0.9023\n",
            "Epoch 18/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.1423 - accuracy: 0.9552 - val_loss: 0.4419 - val_accuracy: 0.8851\n",
            "Epoch 19/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.1818 - accuracy: 0.9408 - val_loss: 0.3699 - val_accuracy: 0.8793\n",
            "Epoch 20/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.1372 - accuracy: 0.9509 - val_loss: 0.4087 - val_accuracy: 0.8851\n",
            "Epoch 21/52\n",
            "22/22 [==============================] - 178s 8s/step - loss: 0.0874 - accuracy: 0.9855 - val_loss: 0.3544 - val_accuracy: 0.8908\n",
            "Epoch 22/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.0670 - accuracy: 0.9913 - val_loss: 0.3380 - val_accuracy: 0.8908\n",
            "Epoch 23/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0588 - accuracy: 0.9957 - val_loss: 0.3592 - val_accuracy: 0.9023\n",
            "Epoch 24/52\n",
            "22/22 [==============================] - 181s 8s/step - loss: 0.0588 - accuracy: 0.9899 - val_loss: 0.3256 - val_accuracy: 0.8851\n",
            "Epoch 25/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0749 - accuracy: 0.9812 - val_loss: 0.3343 - val_accuracy: 0.9080\n",
            "Epoch 26/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.0539 - accuracy: 0.9884 - val_loss: 0.3474 - val_accuracy: 0.8966\n",
            "Epoch 27/52\n",
            "22/22 [==============================] - 178s 8s/step - loss: 0.0455 - accuracy: 0.9986 - val_loss: 0.3187 - val_accuracy: 0.9195\n",
            "Epoch 28/52\n",
            "22/22 [==============================] - 177s 8s/step - loss: 0.0420 - accuracy: 0.9957 - val_loss: 0.2989 - val_accuracy: 0.9080\n",
            "Epoch 29/52\n",
            "22/22 [==============================] - 181s 8s/step - loss: 0.0359 - accuracy: 0.9986 - val_loss: 0.3370 - val_accuracy: 0.8966\n",
            "Epoch 30/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0373 - accuracy: 0.9986 - val_loss: 0.3403 - val_accuracy: 0.9138\n",
            "Epoch 31/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0377 - accuracy: 0.9986 - val_loss: 0.3626 - val_accuracy: 0.9080\n",
            "Epoch 32/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0312 - accuracy: 0.9986 - val_loss: 0.3293 - val_accuracy: 0.9023\n",
            "Epoch 33/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0334 - accuracy: 0.9971 - val_loss: 0.3466 - val_accuracy: 0.9080\n",
            "Epoch 34/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.2980 - val_accuracy: 0.9080\n",
            "Epoch 35/52\n",
            "22/22 [==============================] - 172s 8s/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.8966\n",
            "Epoch 36/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0239 - accuracy: 0.9986 - val_loss: 0.3234 - val_accuracy: 0.9138\n",
            "Epoch 37/52\n",
            "22/22 [==============================] - 181s 8s/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9080\n",
            "Epoch 38/52\n",
            "22/22 [==============================] - 181s 8s/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.3531 - val_accuracy: 0.9080\n",
            "Epoch 39/52\n",
            "22/22 [==============================] - 173s 8s/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.3154 - val_accuracy: 0.9023\n",
            "Epoch 40/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.3257 - val_accuracy: 0.9080\n",
            "Epoch 41/52\n",
            "22/22 [==============================] - 183s 8s/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 0.9080\n",
            "Epoch 42/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.3265 - val_accuracy: 0.9195\n",
            "Epoch 43/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.9080\n",
            "Epoch 44/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 0.8966\n",
            "Epoch 45/52\n",
            "22/22 [==============================] - 173s 8s/step - loss: 0.0158 - accuracy: 0.9986 - val_loss: 0.3403 - val_accuracy: 0.9023\n",
            "Epoch 46/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.3177 - val_accuracy: 0.9080\n",
            "Epoch 47/52\n",
            "22/22 [==============================] - 180s 8s/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3601 - val_accuracy: 0.9080\n",
            "Epoch 48/52\n",
            "22/22 [==============================] - 179s 8s/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 0.9138\n",
            "Epoch 49/52\n",
            "22/22 [==============================] - 176s 8s/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3428 - val_accuracy: 0.9195\n",
            "Epoch 50/52\n",
            "22/22 [==============================] - 175s 8s/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 0.9195\n",
            "Epoch 51/52\n",
            "22/22 [==============================] - 182s 8s/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3585 - val_accuracy: 0.9023\n",
            "Epoch 52/52\n",
            "22/22 [==============================] - 178s 8s/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 0.8908\n",
            "7/7 [==============================] - 44s 6s/step - loss: 0.4190 - accuracy: 0.8986\n",
            "Test accuracy: 0.8986175060272217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming expression_images has shape (num_samples, height, width, channels)\n",
        "# and expression_labels are the corresponding labels\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "expression_images = np.array(expression_images)\n",
        "expression_labels = np.array(expression_labels)\n",
        "\n",
        "# Check the shape of the image data and labels\n",
        "print(\"Expression Images Shape:\", expression_images.shape)\n",
        "print(\"Expression Labels Shape:\", expression_labels.shape)\n",
        "\n",
        "# Obtain the number of class labels\n",
        "num_classes = len(np.unique(expression_labels))\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "expression_labels_one_hot = to_categorical(expression_labels, num_classes=num_classes)\n",
        "\n",
        "# Stratified Splitting of data into training and testing sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    expression_images, expression_labels_one_hot, test_size=0.2, stratify=expression_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Load the ResNet50 model pre-trained on ImageNet data (excluding the top layers)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the ResNet base model\n",
        "model.add(base_model)\n",
        "\n",
        "# Add a global average pooling layer to reduce spatial dimensions\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Add a dense layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add the output layer with softmax activation for classification\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joNYhdhcJ3HC",
        "outputId": "30b1970e-222e-49ba-e909-414b1618da1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression Images Shape: (1083, 224, 224, 3)\n",
            "Expression Labels Shape: (1083,)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 2048)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24114055 (91.99 MB)\n",
            "Trainable params: 526343 (2.01 MB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.save('/content/drive/My Drive/GAN_Models/Resnet_Microexp.h5')\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/My Drive/GAN_Models/Resnet_Microexp.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq-hlkexUewL",
        "outputId": "3aa0ae93-31a8-4539-d619-fd0a5464adb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model('/content/drive/My Drive/GAN_Models/Resnet_Microexp.h5')\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
        "\n",
        "# ... (previous code for model training and evaluation)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_probabilities = model.predict(test_images)\n",
        "predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "# Convert one-hot encoded test labels back to categorical labels\n",
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Print accuracy, precision, recall, F1-score, and UAR\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "print(f'Precision: {precision_score(true_labels, predicted_labels, average=\"weighted\"):.4f}')\n",
        "print(f'Recall: {recall_score(true_labels, predicted_labels, average=\"weighted\"):.4f}')\n",
        "print(f'F1-score: {f1_score(true_labels, predicted_labels, average=\"weighted\"):.4f}')\n",
        "print(f'Unweighted Average Recall (UAR): {balanced_accuracy_score(true_labels, predicted_labels):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NythvKelJqBV",
        "outputId": "f2a8515f-79d4-41ad-b872-30d74001cefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 48s 6s/step\n",
            "Test Accuracy: 0.8986\n",
            "Precision: 0.9154\n",
            "Recall: 0.8986\n",
            "F1-score: 0.9015\n",
            "Unweighted Average Recall (UAR): 0.9091\n"
          ]
        }
      ]
    }
  ]
}